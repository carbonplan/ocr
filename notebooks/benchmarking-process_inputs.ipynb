{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4e24de-ae06-4a5f-9510-6cb83849a976",
   "metadata": {},
   "source": [
    "# Process benchmarking inputs\n",
    "Read in the raw benchmarking inputs and process them to make plottable data.\n",
    "\n",
    "Note, we split this up from the `benchmarking-make_inputs.ipynb` script because this task requires significantly less memory (and therefore a less expensive machine), but still takes some time so we only want to do it once. Operations should be achievable with < 35 GB memory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c61c41f-3930-4807-8b9a-2d9ef58fbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import dask.array as da\n",
    "import geopandas as gpd\n",
    "import icechunk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio.features\n",
    "import seaborn as sns    # plotting\n",
    "from shapely.geometry import Point\n",
    "import xarray as xr\n",
    "\n",
    "from ocr import catalog   # for the riley data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7721ea89-f703-4f4d-8ba4-e85c9aae16e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0xe6bbb8e06fd0>\n",
      "Unclosed connector\n",
      "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0xe6bbb8d25d30>, 1797.26345242)])']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0xe6bbb8e04f50>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0xe6bbb8e07250>\n",
      "Unclosed connector\n",
      "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0xe6bbb8d25f70>, 1797.266821713)])']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0xe6bd5816b890>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0xe6bbb8e04cd0>\n",
      "Unclosed connector\n",
      "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0xe6bbb8d25af0>, 1797.268367675)])']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0xe6bbb8e04910>\n"
     ]
    }
   ],
   "source": [
    "# --- read in dat\n",
    "s3_path = \"s3://carbonplan-ocr/evaluation/benchmark-v0131\"\n",
    "savename = \"benchmarking-input-dat.zarr\"\n",
    "ds = xr.open_zarr(os.path.join(s3_path, savename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0edc93f-e0a2-40dd-b6cf-f4458188460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- filter to four slices \n",
    "#     (CONUS, west, east, testbox) and later (all data, non-burnable)\n",
    "slicenames = {\n",
    "    \"testbox\": {\"minlat\": 42.7, \"maxlat\": 46.3, \"minlon\": -116.8, \"maxlon\": -112.8},\n",
    "    \"CONUS\": None,\n",
    "    \"West of -98\": {\"maxlon\": -98},\n",
    "    \"East of -98\": {\"minlon\": -98},\n",
    "}\n",
    "\n",
    "outdict = {}    # to hold outputs\n",
    "for slc_key, slc_val in slicenames.items():\n",
    "    if slc_key == \"testbox\":\n",
    "        outdict[slc_key] = ds.sel(latitude = slice(slc_val['minlat'], slc_val['maxlat']), \n",
    "                                  longitude = slice(slc_val['minlon'], slc_val['maxlon']))\n",
    "    elif slc_key == \"CONUS\":\n",
    "        outdict[slc_key] = ds.copy()\n",
    "\n",
    "    elif slc_key == \"West of -98\":\n",
    "        outdict[slc_key] = ds.where(ds['longitude'] < slc_val['maxlon'], drop=True)\n",
    "\n",
    "    elif slc_key == \"East of -98\": \n",
    "        outdict[slc_key] = ds.where(ds['longitude'] >= slc_val['minlon'], drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d7f87-2e08-49ad-baaf-25a7168435a8",
   "metadata": {},
   "source": [
    "# Create N bins of data (for plotting distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5dd24f0-c13e-4af6-a499-0e73721d36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- set bin bounds\n",
    "n_bins = 10000   # this number controls distribution resolution\n",
    "bp_range = (0, 0.14)  # to confirm max: dsslice['burn_probability_2011'].max(skipna=True).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd1d398-b528-424b-8768-6f4961450d87",
   "metadata": {},
   "source": [
    "## Bins across burn mask conditions\n",
    "Weighted by cell area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12921ca2-4242-42f2-879d-965d71fd3536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now solving: testbox\n",
      "CHECK 1: Masks defined\n",
      "CHECK 2: All data wtd hists done \n",
      "CHECK 3: Unburnable data wtd hists done \n",
      "CHECK 4: Hists in memory \n",
      "CHECK 5: Hists normalized\n",
      "Now solving: CONUS\n",
      "CHECK 1: Masks defined\n",
      "CHECK 2: All data wtd hists done \n",
      "CHECK 3: Unburnable data wtd hists done \n",
      "CHECK 4: Hists in memory \n",
      "CHECK 5: Hists normalized\n",
      "Now solving: West of -98\n",
      "CHECK 1: Masks defined\n",
      "CHECK 2: All data wtd hists done \n",
      "CHECK 3: Unburnable data wtd hists done \n",
      "CHECK 4: Hists in memory \n",
      "CHECK 5: Hists normalized\n",
      "Now solving: East of -98\n",
      "CHECK 1: Masks defined\n",
      "CHECK 2: All data wtd hists done \n",
      "CHECK 3: Unburnable data wtd hists done \n",
      "CHECK 4: Hists in memory \n",
      "CHECK 5: Hists normalized\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da \n",
    "\n",
    "df_dict = {}   # to hold results\n",
    "\n",
    "# helper function to compute weighted histogram lazily\n",
    "def weighted_histogram(data, weights, n_bins, data_range):\n",
    "    # make sure chunks match\n",
    "    weights = weights.rechunk(data.chunks)\n",
    "    hist, bins = da.histogram(data, bins=n_bins, range=data_range, weights=weights)\n",
    "    return hist, bins\n",
    "\n",
    "# --- run the loop\n",
    "for dsname, dsslice in outdict.items():\n",
    "\n",
    "    # track progress\n",
    "    print(f\"Now solving: {dsname}\")\n",
    "    \n",
    "    # define variables\n",
    "    bp = dsslice['burn_probability_2011']\n",
    "    mask = dsslice['burn_mask']\n",
    "    riley_b_mask = dsslice['riley_burnable_mask']\n",
    "    cell_area = dsslice['cell_area']  # assumed same shape as bp\n",
    "    \n",
    "    # define masks\n",
    "    burned = mask == 1\n",
    "    unburned = mask == 0\n",
    "    riley_unburnable = riley_b_mask == 0\n",
    "    \n",
    "    print(\"CHECK 1: Masks defined\")\n",
    "    \n",
    "    # --- compute histograms lazily\n",
    "    # ALL DATA\n",
    "    burned_hist, bins = weighted_histogram(bp.where(burned).data,\n",
    "                                           cell_area.where(burned).data,\n",
    "                                           n_bins, bp_range)\n",
    "    unburned_hist, _ = weighted_histogram(bp.where(unburned).data,\n",
    "                                          cell_area.where(unburned).data,\n",
    "                                          n_bins, bp_range)\n",
    "    \n",
    "    print(\"CHECK 2: All data wtd hists done \")\n",
    "    \n",
    "    # UNBURNABLE ONLY\n",
    "    burned_hist_unburnable, _ = weighted_histogram(\n",
    "        bp.where(burned & riley_unburnable).data,\n",
    "        cell_area.where(burned & riley_unburnable).data,\n",
    "        n_bins, bp_range\n",
    "    )\n",
    "    unburned_hist_unburnable, _ = weighted_histogram(\n",
    "        bp.where(unburned & riley_unburnable).data,\n",
    "        cell_area.where(unburned & riley_unburnable).data,\n",
    "        n_bins, bp_range\n",
    "    )\n",
    "    \n",
    "    print(\"CHECK 3: Unburnable data wtd hists done \")\n",
    "    \n",
    "    # --- bring histograms into memory\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    burned_hist, unburned_hist, burned_hist_unburnable, unburned_hist_unburnable = da.compute(\n",
    "        burned_hist, unburned_hist, burned_hist_unburnable, unburned_hist_unburnable\n",
    "    )\n",
    "    \n",
    "    print(\"CHECK 4: Hists in memory \")\n",
    "    \n",
    "    # --- normalize to density\n",
    "    burned_density = burned_hist / burned_hist.sum()\n",
    "    unburned_density = unburned_hist / unburned_hist.sum()\n",
    "    burned_density_unburnable = burned_hist_unburnable / burned_hist_unburnable.sum()\n",
    "    unburned_density_unburnable = unburned_hist_unburnable / unburned_hist_unburnable.sum()\n",
    "    \n",
    "    print(\"CHECK 5: Hists normalized\")\n",
    "    \n",
    "    # --- combine into single dataframe\n",
    "    tmpdict = {\n",
    "        \"bin_centers\": bin_centers,\n",
    "        \"burned_BPdensity\": burned_density,\n",
    "        \"unburned_BPdensity\": unburned_density,\n",
    "        \"burned_BPdensity_NB\": burned_density_unburnable,\n",
    "        \"unburned_BPdensity_NB\": unburned_density_unburnable,\n",
    "    }\n",
    "    \n",
    "    df_dict[dsname] = pd.DataFrame(tmpdict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42311652-62b3-45bc-a43e-4c4fb5509dee",
   "metadata": {},
   "source": [
    "#### Save mask dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "debede8c-a6f3-4eff-b982-7a4d91b513fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem()\n",
    "\n",
    "for name, df in df_dict.items():\n",
    "    path = f\"s3://carbonplan-ocr/evaluation/benchmark-v0131/benchmarking-processed/{name}_WTD_maskdf.parquet\"\n",
    "    with s3.open(path, 'wb') as f:\n",
    "        df.to_parquet(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c4c71-4a1c-4a0f-9e7d-35d13db5e4c9",
   "metadata": {},
   "source": [
    "## Bins across burn sum conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fffbbc8-9d25-4daf-ae99-7a862df653fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RUN ONCE: get the max number of burns\n",
    "# **** N = 156 (!!) ****\n",
    "# nburn_max = ds['burn_sum'].max(skipna=True).compute()\n",
    "# ---\n",
    "# (earlier testing indicates that pixels where nburn > 20 are very rare, \n",
    "#  so we'll split into 0-20 for now)\n",
    "nburn_arr = np.arange(0, 21, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b88926cb-8174-42dd-b5d0-c7871df11a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now solving: testbox\n",
      "Now solving: CONUS\n",
      "n fires: 3"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m     burned_density = burned_hist[\u001b[32m0\u001b[39m] / burned_hist[\u001b[32m0\u001b[39m].sum()\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# [ UNBURNABLE ONLY ] \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m burned_hist_unburnable = \u001b[43mda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mburned_hist_unburnable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# normalize counts to density\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(invalid=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, divide=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m): \u001b[38;5;66;03m# to handle cases where no pixel has this number of burns\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/coiled/env/lib/python3.13/site-packages/dask/base.py:681\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     expr = expr.optimize()\n\u001b[32m    679\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/coiled/env/lib/python3.13/queue.py:199\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m    201\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ShutDown\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/coiled/env/lib/python3.13/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import dask.array as da \n",
    "\n",
    "ds_dict_burnsum = {}   # to hold results\n",
    "\n",
    "for dsname, dsslice in outdict.items():\n",
    "\n",
    "    # track progress\n",
    "    print(f\"Now solving: {dsname}\")\n",
    "\n",
    "    # define variables\n",
    "    bp = dsslice['burn_probability_2011']\n",
    "    burnsum = dsslice['burn_sum']\n",
    "    riley_b_mask = dsslice['riley_burnable_mask']\n",
    "    riley_burnable = riley_b_mask == 1\n",
    "    riley_unburnable = riley_b_mask == 0\n",
    "\n",
    "    dfs_nburn = {} # empty dict to hold one dataframe per nburn_arr\n",
    "    \n",
    "    for nfire in nburn_arr:\n",
    "        print(f\"n fires: {nfire}\", end=\"\\r\")\n",
    "        \n",
    "        # --- create a mask for just these burns\n",
    "        this_burnsum_mask = burnsum == nfire\n",
    "        \n",
    "        # --- compute histograms lazily\n",
    "        # [ ALL DATA ] \n",
    "        burned_hist, bins = da.histogram(bp.where(this_burnsum_mask).data, bins=n_bins, range=bp_range)\n",
    "        # [ UNBURNABLE ONLY ] \n",
    "        burned_hist_unburnable, bins = da.histogram(bp.where((this_burnsum_mask) & (riley_unburnable)).data, bins=n_bins, range=bp_range)\n",
    "        \n",
    "        # bring just the histograms into memory\n",
    "        bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "        # [ ALL DATA ] \n",
    "        burned_hist = da.compute(burned_hist)\n",
    "        # normalize counts to density\n",
    "        with np.errstate(invalid='ignore', divide='ignore'): # to handle cases where no pixel has this number of burns\n",
    "            burned_density = burned_hist[0] / burned_hist[0].sum()\n",
    "            \n",
    "        # [ UNBURNABLE ONLY ] \n",
    "        burned_hist_unburnable = da.compute(burned_hist_unburnable)\n",
    "        # normalize counts to density\n",
    "        with np.errstate(invalid='ignore', divide='ignore'): # to handle cases where no pixel has this number of burns\n",
    "            burned_density_unburnable = burned_hist_unburnable[0] / burned_hist_unburnable[0].sum()\n",
    "        \n",
    "        # --- combine into single dataframe \n",
    "        tmpdict = {\n",
    "            \"n_burns\": nfire,\n",
    "            \"bin\": bin_centers,\n",
    "            \"burned_BPdensity\": burned_density,\n",
    "            \"burned_BPdensity_NB\": burned_density_unburnable,\n",
    "        }\n",
    "        \n",
    "        dfs_nburn[str(nfire)] = pd.DataFrame(tmpdict)\n",
    "\n",
    "    # --- create a processed dataset\n",
    "    #     with dims = n_burns, bin\n",
    "    # dict to list\n",
    "    dfs_list = list(dfs_nburn.values())\n",
    "    \n",
    "    # stack each data column into a 2D array\n",
    "    data_vars = {}\n",
    "    for col in dfs_list[0].columns:\n",
    "        if col in [\"n_burns\", \"bin\"]:\n",
    "            continue\n",
    "        data_vars[col] = ((\"n_burns\", \"bin\"), [df[col].values for df in dfs_list])\n",
    "    \n",
    "    # build the dataset\n",
    "    tmpds = xr.Dataset(\n",
    "        data_vars=data_vars,\n",
    "        coords={\n",
    "            \"n_burns\": nburn_arr,\n",
    "            \"bin\": bin_centers,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ds_dict_burnsum[dsname] = tmpds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68947945-6132-43ff-9081-262194792524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/coiled/env/lib/python3.13/site-packages/zarr/api/asynchronous.py:244: ZarrUserWarning: Consolidated metadata is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem()\n",
    "\n",
    "for name, ds in ds_dict_burnsum.items():\n",
    "    path = f\"s3://carbonplan-ocr/evaluation/benchmarking-processed/{name}_WTD_sumds.zarr\"\n",
    "    ds.to_zarr(store=s3.get_mapper(path), mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d034545-5ec8-41fa-ae68-cba8b12ed9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da \n",
    "\n",
    "# --- helper for weighted histogram (lazy)\n",
    "def weighted_histogram(data, weights, n_bins, data_range):\n",
    "    weights = weights.rechunk(data.chunks)\n",
    "    hist, bins = da.histogram(data, bins=n_bins, range=data_range, weights=weights)\n",
    "    return hist, bins\n",
    "\n",
    "    \n",
    "ds_dict_burnsum = {}   # to hold results\n",
    "\n",
    "for dsname, dsslice in outdict.items():\n",
    "\n",
    "    # track progress\n",
    "    print(f\"Now solving: {dsname}\")\n",
    "\n",
    "    # define variables\n",
    "    bp = dsslice['burn_probability_2011']\n",
    "    burnsum = dsslice['burn_sum']\n",
    "    riley_b_mask = dsslice['riley_burnable_mask']\n",
    "    riley_burnable = riley_b_mask == 1\n",
    "    riley_unburnable = riley_b_mask == 0\n",
    "\n",
    "    dfs_nburn = {} # empty dict to hold one dataframe per nburn_arr\n",
    "        \n",
    "    for nfire in nburn_arr:\n",
    "        print(f\"n fires: {nfire}\", end=\"\\r\")\n",
    "        \n",
    "        # --- create a mask for just these burns\n",
    "        this_burnsum_mask = burnsum == nfire\n",
    "        \n",
    "        # --- prepare masked data lazily\n",
    "        bp_masked_all = bp.where(this_burnsum_mask).data\n",
    "        area_masked_all = cell_area.where(this_burnsum_mask).data.rechunk(bp_masked_all.chunks)\n",
    "        \n",
    "        bp_masked_unburnable = bp.where(this_burnsum_mask & riley_unburnable).data\n",
    "        area_masked_unburnable = cell_area.where(this_burnsum_mask & riley_unburnable).data.rechunk(bp_masked_unburnable.chunks)\n",
    "        \n",
    "        # --- compute histograms lazily\n",
    "        burned_hist, bins = da.histogram(bp_masked_all, bins=n_bins, range=bp_range, weights=area_masked_all)\n",
    "        burned_hist_unburnable, _ = da.histogram(bp_masked_unburnable, bins=n_bins, range=bp_range, weights=area_masked_unburnable)\n",
    "        \n",
    "        # --- compute and normalize\n",
    "        burned_hist, burned_hist_unburnable = da.compute(burned_hist, burned_hist_unburnable)\n",
    "        \n",
    "        with np.errstate(invalid='ignore', divide='ignore'):\n",
    "            burned_density = burned_hist / burned_hist.sum() if burned_hist.sum() > 0 else np.zeros_like(burned_hist)\n",
    "            burned_density_unburnable = (\n",
    "                burned_hist_unburnable / burned_hist_unburnable.sum()\n",
    "                if burned_hist_unburnable.sum() > 0\n",
    "                else np.zeros_like(burned_hist_unburnable)\n",
    "            )\n",
    "        \n",
    "        # --- store results\n",
    "        bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "        tmpdict = {\n",
    "            \"n_burns\": nfire,\n",
    "            \"bin\": bin_centers,\n",
    "            \"burned_BPdensity\": burned_density,\n",
    "            \"burned_BPdensity_NB\": burned_density_unburnable,\n",
    "        }\n",
    "        \n",
    "        dfs_nburn[str(nfire)] = pd.DataFrame(tmpdict)\n",
    "        \n",
    "    # --- create a processed dataset\n",
    "    #     with dims = n_burns, bin\n",
    "    # dict to list\n",
    "    dfs_list = list(dfs_nburn.values())\n",
    "    \n",
    "    # stack each data column into a 2D array\n",
    "    data_vars = {}\n",
    "    for col in dfs_list[0].columns:\n",
    "        if col in [\"n_burns\", \"bin\"]:\n",
    "            continue\n",
    "        data_vars[col] = ((\"n_burns\", \"bin\"), [df[col].values for df in dfs_list])\n",
    "    \n",
    "    # build the dataset\n",
    "    tmpds = xr.Dataset(\n",
    "        data_vars=data_vars,\n",
    "        coords={\n",
    "            \"n_burns\": nburn_arr,\n",
    "            \"bin\": bin_centers,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ds_dict_burnsum[dsname] = tmpds\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
