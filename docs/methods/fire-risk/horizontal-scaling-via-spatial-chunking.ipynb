{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56fcda59-64e2-4622-a0f2-c7d889ebd886",
   "metadata": {},
   "source": [
    "# Horizontal Scaling via Spatial Chunking\n",
    "\n",
    "## Overview\n",
    "\n",
    "Open Climate Risk (OCR) uses a spatial chunking system that enables parallel processing of large-scale CONUS fire risk datasets at 30m resolution. Similar to the serverless approach described in [\"A Serverless Approach to Building Planetary-Scale EO Datacubes\"](https://earthmover.io/blog/serverless-datacube-pipeline), OCR breaks down the complete CONUS dataset into manageable, independently processable regions.\n",
    "\n",
    "This approach provides several key advantages:\n",
    "\n",
    "- **Parallelization**: Process hundreds of regions simultaneously across distributed compute resources\n",
    "- **Fault tolerance**: Failed regions can be reprocessed independently without affecting others\n",
    "- **Resource efficiency**: Match chunk sizes to available memory and compute resources\n",
    "- **Incremental processing**: Process and commit regions as they complete, enabling progress tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b122efdb-8792-46c3-b2d8-ace26f7d2ed5",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "The spatial chunking system is built around the `ChunkingConfig` class, which provides:\n",
    "\n",
    "1. **Chunk definition and layout**: Defines the spatial grid dividing CONUS into processable regions\n",
    "2. **Spatial indexing**: Rapidly identifies chunks intersecting with regions of interest\n",
    "3. **Coordinate transformations**: Converts between pixel indices, geographic coordinates, and chunk IDs\n",
    "4. **Visualization capabilities**: Displays chunk layouts and spatial relationships\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[CONUS 30m Dataset] --> B[ChunkingConfig]\n",
    "    B --> C[Spatial Grid Definition]\n",
    "    B --> D[Coordinate System]\n",
    "    B --> E[Region ID Mapping]\n",
    "    \n",
    "    C --> F[595 Processing Regions]\n",
    "    D --> G[EPSG:4326 Transform]\n",
    "    E --> H[Region ID ↔ Chunk ID ↔ Lat/Lon Slices]\n",
    "    \n",
    "    F --> I[Parallel Processing]\n",
    "    G --> I\n",
    "    H --> I\n",
    "    \n",
    "    I --> J[Region 0: y0_x0]\n",
    "    I --> K[Region 1: y0_x1]\n",
    "    I --> L[Region N: yM_xN]\n",
    "    \n",
    "    J --> M[Icechunk Store]\n",
    "    K --> M\n",
    "    L --> M\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style M fill:#ffe1e1\n",
    "    style I fill:#fff4e1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb20df7",
   "metadata": {},
   "source": [
    "## Chunking Configuration\n",
    "\n",
    "Let's explore the chunking configuration using the `ChunkingConfig` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d6b48b-3795-47bd-9a89-66097d95bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocr.config import ChunkingConfig\n",
    "\n",
    "config = ChunkingConfig(debug=True)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd5c195-d0e4-494f-99ca-371325fc6d3f",
   "metadata": {},
   "source": [
    "### Template Dataset\n",
    "\n",
    "The chunking system uses the `scott-et-al-2024-30m-4326` dataset as a template to establish the spatial reference framework:\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| **Resolution** | 30m (~0.0002778°) |\n",
    "| **Projection** | EPSG:4326 (WGS84) |\n",
    "| **Total Chunks** | 595 regions |\n",
    "| **Chunk Size** | 6000 × 4500 pixels |\n",
    "| **Dimensions** | latitude × longitude |\n",
    "\n",
    "`ChunkingConfig` uses this dataset as a template to compute properties (e.g. CONUS bounds, `transform`, chunk information) that are used to convert between chunk IDs and spatial bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5cedc-abed-407c-931a-bd538639c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.ds.odc.geobox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4005943",
   "metadata": {},
   "source": [
    "### Coordinate System\n",
    "\n",
    "The system maintains an affine transformation matrix that enables bidirectional conversion between:\n",
    "\n",
    "- **Pixel indices** (array coordinates)\n",
    "- **Geographic coordinates** (EPSG:4326 longitude/latitude)\n",
    "- **Chunk IDs** (region identifiers)\n",
    "\n",
    "This transformation is crucial for:\n",
    "- Converting bounding boxes to chunk selections\n",
    "- Extracting geographic extents from pixel regions\n",
    "- Validating spatial queries and data alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e01ff-32ba-45fd-9a53-c0044fbcb749",
   "metadata": {},
   "source": [
    "## Visualizing the Chunking System\n",
    "\n",
    "### All Chunks Layout\n",
    "\n",
    "The `.plot_all_chunks()` method allows us to visualize the grid layout of our zarr/dask chunks and how they align with the CONUS geography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f4b7c-c2dd-45e1-ae85-b3720df7ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.plot_all_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91310da-b2f9-4cfc-8bb8-a81f35f54112",
   "metadata": {},
   "source": [
    "In the plot above, we can see the chunk layout of the `USFS-wildfire-risk-communities` dataset. The dataset has 595 chunks, each with size (6000, 4500). The chunks are arranged in a grid layout, with each chunk covering a specific geographic area.\n",
    "\n",
    "Let's examine the dataset structure and verify the number of chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f20c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of dask/zarr chunks: {config.ds.CRPS.data.npartitions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c03253-298f-47b7-9eb2-635b69460ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.region_id_to_latlon_slices('y5_x14')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a25ab0c",
   "metadata": {},
   "source": [
    "## Region Identification System\n",
    "\n",
    "### Region ID Format\n",
    "\n",
    "Each processing region is identified by a unique string ID:\n",
    "\n",
    "```text\n",
    "y{iy}_x{ix}\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `iy`: Index along the y-dimension (latitude), ranging from 0 to number of y-chunks - 1\n",
    "- `ix`: Index along the x-dimension (longitude), ranging from 0 to number of x-chunks - 1\n",
    "\n",
    "**Example**: `y5_x14` refers to the chunk at row 5, column 14 of the spatial grid.\n",
    "\n",
    "### Region ID Lookup Process\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Region ID<br/>y5_x14] --> B[Chunk Index<br/>iy=5, ix=14]\n",
    "    B --> C[Pixel Slices<br/>y: 30000-36000<br/>x: 63000-67500]\n",
    "    C --> D[Geographic Bounds<br/>lat: 31.6° - 33.5°<br/>lon: -108.9° - -107.5°]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style D fill:#ffe1e1\n",
    "```\n",
    "\n",
    "### Valid Region Filtering\n",
    "\n",
    "Not all regions in the grid contain valid data. The system maintains a list of valid region IDs that:\n",
    "- Contain non-null data (not entirely ocean or outside CONUS)\n",
    "- Have been precomputed to avoid processing empty regions\n",
    "- Are checked during pipeline execution to skip invalid regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a270f2b0",
   "metadata": {},
   "source": [
    "## Spatial Indexing\n",
    "\n",
    "### Bounding Box Queries\n",
    "\n",
    "The chunking system can efficiently identify which chunks intersect with arbitrary geographic bounding boxes. Let's demonstrate this with several U.S. states:\n",
    "\n",
    "We've defined bounding boxes for three different regions:\n",
    "- Colorado (co_bbox): A rectangular region covering the state of Colorado\n",
    "- California (ca_bbox): A rectangular region covering the state of California\n",
    "- Arkansas (ar_bbox): A rectangular region covering the state of Arkansas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e21ecc-9666-499c-8a11-b837ea180152",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_bbox = config.bbox_from_wgs84(-109.059196, 36.992751, -102.042126, 41.001982)\n",
    "ca_bbox = config.bbox_from_wgs84(\n",
    "    -124.41060660766607, 32.5342307609976, -114.13445790587905, 42.00965914828148\n",
    ")\n",
    "ar_bbox = config.bbox_from_wgs84(\n",
    "    -94.61946646626465, 33.00413641175411, -89.65547287402873, 36.49965029279292\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc1fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display the bounding boxes to see their coordinates\n",
    "print('Colorado bbox:', co_bbox)\n",
    "print('California bbox:', ca_bbox)\n",
    "print('Arkansas bbox:', ar_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e455480",
   "metadata": {},
   "source": [
    "### Spatial Query Workflow\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant User\n",
    "    participant Config as ChunkingConfig\n",
    "    participant Index as Spatial Index\n",
    "    participant Store as Data Store\n",
    "    \n",
    "    User->>Config: bbox_from_wgs84(lon, lat bounds)\n",
    "    Config->>Index: Compute intersecting chunks\n",
    "    Index->>Index: Transform bounds to pixel space\n",
    "    Index->>Index: Identify overlapping chunks\n",
    "    Index-->>Config: Return chunk IDs\n",
    "    Config-->>User: ['y7_x12', 'y8_x13', ...]\n",
    "    \n",
    "    User->>Store: Process only selected chunks\n",
    "    Store-->>User: Targeted results\n",
    "```\n",
    "\n",
    "The next cell will:\n",
    "\n",
    "1. Identify all chunks that intersect with each bounding box using `get_chunks_for_bbox()`\n",
    "2. Visualize these chunks on the CONUS map to demonstrate how our chunking system can efficiently select only the data chunks needed for specific geographic regions\n",
    "\n",
    "This illustrates a key advantage of the spatial chunking system; rather than processing the entire CONUS dataset, we can quickly identify and process only the chunks that contain our regions of interest. This is particularly useful for:\n",
    "\n",
    "- **Targeted processing**: Only compute regions of interest\n",
    "- **Debugging workflows**: Re-run specific geographic areas\n",
    "- **Incremental updates**: Process new regions without reprocessing everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02868455-b660-4b23-ae1d-dfb07333e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_chunks = config.get_chunks_for_bbox(co_bbox)\n",
    "ca_chunks = config.get_chunks_for_bbox(ca_bbox)\n",
    "ar_chunks = config.get_chunks_for_bbox(ar_bbox)\n",
    "config.visualize_chunks_on_conus(chunks=ca_chunks + co_chunks + ar_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a801d",
   "metadata": {},
   "source": [
    "### Fine-Grained Spatial Queries\n",
    "\n",
    "In the cell below, we identify the chunks that intersect with the Eaton fire in California, demonstrating the system's ability to work at very fine spatial scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ffc114-f237-4f98-bf13-18062170212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eaton_fire_bbox = config.bbox_from_wgs84(\n",
    "    -118.16359507363843, 34.1609756217009, -118.01833468855428, 34.23216929604394\n",
    ")\n",
    "eaton_fire_chunks = config.get_chunks_for_bbox(eaton_fire_bbox)\n",
    "config.visualize_chunks_on_conus(\n",
    "    eaton_fire_chunks, highlight_chunks=eaton_fire_chunks, include_all_chunks=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb31ce-18d3-4432-9e8a-7c981c426b87",
   "metadata": {},
   "source": [
    "## Parallel Processing Framework\n",
    "\n",
    "### Processing Pipeline\n",
    "\n",
    "Like the serverless approach described in the [blog post](https://earthmover.io/blog/serverless-datacube-pipeline), our system enables parallel processing of chunks.\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"Input\"\n",
    "        A[Valid Region IDs]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Orchestration\"\n",
    "        B[Task Scheduler<br/>Coiled/Dask]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Parallel Workers\"\n",
    "        C1[Worker 1<br/>Region y0_x5]\n",
    "        C2[Worker 2<br/>Region y1_x8]\n",
    "        C3[Worker 3<br/>Region y2_x12]\n",
    "        CN[Worker N<br/>Region yM_xN]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Processing\"\n",
    "        D1[Calculate Risk<br/>Region y0_x5]\n",
    "        D2[Calculate Risk<br/>Region y1_x8]\n",
    "        D3[Calculate Risk<br/>Region y2_x12]\n",
    "        DN[Calculate Risk<br/>Region yM_xN]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Storage\"\n",
    "        E1[Commit to Icechunk<br/>Region y0_x5]\n",
    "        E2[Commit to Icechunk<br/>Region y1_x8]\n",
    "        E3[Commit to Icechunk<br/>Region y2_x12]\n",
    "        EN[Commit to Icechunk<br/>Region yM_xN]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Output\"\n",
    "        G[Icechunk Store<br/>Raster Data]\n",
    "        H[GeoParquet Files<br/>Vector Data]\n",
    "    end\n",
    "    \n",
    "    A --> B\n",
    "    B --> C1 & C2 & C3 & CN\n",
    "    C1 --> D1 --> E1\n",
    "    C2 --> D2 --> E2\n",
    "    C3 --> D3 --> E3\n",
    "    CN --> DN --> EN\n",
    "    E1 & E2 & E3 & EN --> G\n",
    "    E1 & E2 & E3 & EN --> H\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff4e1\n",
    "    style G fill:#ffe1e1\n",
    "    style H fill:#ffe1e1\n",
    "```\n",
    "\n",
    "This means that we can:\n",
    "\n",
    "- Create a template zarr store for the entire CONUS dataset\n",
    "- Each worker processes its assigned region independently and writes its own data directly to Icechunk\n",
    "- Because our chunking system ensures that chunks are aligned with the CONUS geography, we can use xarray's `to_zarr(region='auto')` method to automatically determine the region for each chunk\n",
    "- Each worker creates its own commit for the region it processed, enabling parallel writes without conflicts\n",
    "\n",
    "### Implementation Pattern\n",
    "\n",
    "Below is an example of how this might look in practice:\n",
    "\n",
    "```python\n",
    "def process_region(region_id: str, config: OCRConfig):\n",
    "    \"\"\"Process a single region independently\"\"\"\n",
    "    # 1. Get spatial bounds for this region\n",
    "    y_slice, x_slice = config.chunking.region_id_to_latlon_slices(region_id)\n",
    "    \n",
    "    # 2. Calculate wind-adjusted risk for this region\n",
    "    ds = calculate_wind_adjusted_risk(y_slice=y_slice, x_slice=x_slice)\n",
    "    \n",
    "    # 3. Each worker writes its own data to Icechunk with automatic region detection\n",
    "    config.icechunk.insert_region_uncooperative(ds, region_id=region_id)\n",
    "    \n",
    "    # 4. Sample buildings and write vector data\n",
    "    buildings_gdf = sample_risk_to_buildings(ds=ds)\n",
    "    buildings_gdf.to_parquet(f'{region_id}.parquet')\n",
    "    \n",
    "    return region_id\n",
    "\n",
    "# Execute in parallel across all valid regions\n",
    "# Each worker independently processes and writes its region\n",
    "results = []\n",
    "for region_id in config.chunking.valid_region_ids:\n",
    "    result = process_region(region_id, config)\n",
    "    results.append(result)\n",
    "```\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Independence** | Each region processes without dependencies on others |\n",
    "| **Parallel Writes** | Each worker writes its own region data directly to Icechunk |\n",
    "| **Idempotency** | Regions can be reprocessed safely; commits track completion |\n",
    "| **Scalability** | Add more workers to process more regions simultaneously |\n",
    "| **Progress Tracking** | Each worker's commit logs the completed region for restart capability |\n",
    "| **Error Isolation** | Failed regions don't affect successful ones |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951cbfd7",
   "metadata": {},
   "source": [
    "## Fault Tolerance\n",
    "\n",
    "### Restart Capability\n",
    "\n",
    "The system tracks processed regions via Icechunk commit messages:\n",
    "\n",
    "```python\n",
    "# Check which regions are already processed\n",
    "processed = config.icechunk.processed_regions()\n",
    "\n",
    "# Filter to unprocessed regions only\n",
    "status = config.select_region_ids(\n",
    "    region_ids=all_valid_regions,\n",
    "    all_region_ids=True\n",
    ")\n",
    "to_process = status.unprocessed_valid_region_ids\n",
    "\n",
    "# Resume processing from where it left off\n",
    "for region_id in to_process:\n",
    "    process_region(region_id, config)\n",
    "```\n",
    "\n",
    "### Error Handling\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Start Processing Region] --> B{Region Valid?}\n",
    "    B -->|No| C[Skip]\n",
    "    B -->|Yes| D{Already Processed?}\n",
    "    D -->|Yes| E[Skip]\n",
    "    D -->|No| F[Process Region]\n",
    "    F --> G{Success?}\n",
    "    G -->|Yes| H[Commit to Store]\n",
    "    G -->|No| I[Log Error]\n",
    "    I --> J[Continue to Next Region]\n",
    "    H --> K[Update Progress]\n",
    "    \n",
    "    style C fill:#ffd4d4\n",
    "    style E fill:#fff4d4\n",
    "    style H fill:#d4ffd4\n",
    "    style I fill:#ffd4d4\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- **Partial completion**: Successfully processed regions persist even if others fail\n",
    "- **Retry logic**: Failed regions can be identified and reprocessed\n",
    "- **No cascading failures**: One region's error doesn't affect others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12455372",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Region Selection\n",
    "\n",
    "1. **Validate region IDs**: Always check against `valid_region_ids` before processing\n",
    "2. **Check processed status**: Skip already-completed regions to avoid duplicate work\n",
    "3. **Use spatial queries**: When debugging, target specific geographic areas\n",
    "\n",
    "### Processing Configuration\n",
    "\n",
    "1. **Match chunk size to memory**: Ensure workers have sufficient RAM for chunk size\n",
    "2. **Monitor progress**: Track commit messages to verify completion\n",
    "3. **Handle edge cases**: Smaller chunks near CONUS boundaries may need special handling\n",
    "\n",
    "### Debugging Workflow\n",
    "\n",
    "1. **Visualize target area**: Use `visualize_chunks_on_conus()` to identify relevant chunks\n",
    "2. **Process subset**: Test on 2-3 chunks before full CONUS run\n",
    "3. **Verify outputs**: Check both Icechunk commits and GeoParquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd26ca",
   "metadata": {},
   "source": [
    "## Related Documentation\n",
    "\n",
    "-   [Data Schema](../../../reference/data-schema): Structure of output datasets\n",
    "-   [Deployment](../../../reference/deployment): Infrastructure for distributed processing\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
